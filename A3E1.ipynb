{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbHXLFjUwjNZJQL3/ne52Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","import torchvision\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","\n","batch_len = 4\n","\n","training_data = datasets.MNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=torchvision.transforms.Compose([torchvision.transforms.Resize(32),\n","                         torchvision.transforms.ToTensor()])\n",")\n","\n","test_data = datasets.MNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=torchvision.transforms.Compose([torchvision.transforms.Resize(32),\n","                         torchvision.transforms.ToTensor()])\n",")\n","\n","training_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_len, shuffle=True, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_len, shuffle=False, num_workers=2)"],"metadata":{"id":"MglqEPnqN3cJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697156195915,"user_tz":240,"elapsed":7452,"user":{"displayName":"刘亦凡","userId":"09216365541576887674"}},"outputId":"c778a173-5f03-40ce-a133-99ccfd1e2d64"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 80954479.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 94637260.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 20589750.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 15718258.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n"]}]},{"cell_type":"code","source":["print(len(training_data),len(training_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrZf6IytQgGM","executionInfo":{"status":"ok","timestamp":1697156196967,"user_tz":240,"elapsed":1057,"user":{"displayName":"刘亦凡","userId":"09216365541576887674"}},"outputId":"f17a6891-efc0-4e30-a20d-4d6291f9a187"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["60000 15000\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zttQglBndkDJ","executionInfo":{"status":"ok","timestamp":1697156196968,"user_tz":240,"elapsed":3,"user":{"displayName":"刘亦凡","userId":"09216365541576887674"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # nn.Conv2d(input image channel, output channels, square convolution)\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 64, 3, stride=1, padding=1)\n","        self.conv1_bn = nn.BatchNorm2d(64)\n","\n","        self.conv2 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n","        self.conv2_bn = nn.BatchNorm2d(128)\n","\n","        self.conv3 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n","        self.conv3_bn = nn.BatchNorm2d(256)\n","\n","        self.conv4 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n","        self.conv4_bn = nn.BatchNorm2d(256)\n","\n","        self.conv5 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n","        self.conv5_bn = nn.BatchNorm2d(512)\n","\n","        self.conv6 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n","        self.conv6_bn = nn.BatchNorm2d(512)\n","\n","        self.conv7 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n","        self.conv7_bn = nn.BatchNorm2d(512)\n","\n","        self.conv8 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n","        self.conv8_bn = nn.BatchNorm2d(512)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(512, 4096)\n","        self.fc1_dp = nn.Dropout(p=0.5)\n","\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.fc2_dp = nn.Dropout(p=0.5)\n","\n","        self.fc3 = nn.Linear(4096, 10)\n","\n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1_bn(self.conv1(x))), 2)\n","        x = F.max_pool2d(F.relu(self.conv2_bn(self.conv2(x))), 2)\n","        x = F.relu(self.conv3_bn(self.conv3(x)))\n","        x = F.max_pool2d(F.relu(self.conv4_bn(self.conv4(x))), 2)\n","        x = F.relu(self.conv5_bn(self.conv5(x)))\n","        x = F.max_pool2d(F.relu(self.conv6_bn(self.conv6(x))), 2)\n","        x = F.relu(self.conv7_bn(self.conv7(x)))\n","        x = F.max_pool2d(F.relu(self.conv8_bn(self.conv8(x))), 2)\n","\n","        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n","\n","        x = self.fc1_dp(F.relu(self.fc1(x)))\n","        x = self.fc2_dp(F.relu(self.fc2(x)))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","source":["model_E1 = Net()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model_E1.parameters(), lr=0.01)"],"metadata":{"id":"GPsuyx9v3y3e","executionInfo":{"status":"ok","timestamp":1697156198966,"user_tz":240,"elapsed":1008,"user":{"displayName":"刘亦凡","userId":"09216365541576887674"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def train_one_epoch(epoch_index, tb_writer):\n","    running_loss = 0.\n","    last_loss = 0.\n","    total_loss = 0.\n","    train_correct = 0\n","\n","    # Here, we use enumerate(training_loader) instead of\n","    # iter(training_loader) so that we can track the batch\n","    # index and do some intra-epoch reporting\n","    for i, data in enumerate(training_loader):\n","        # Every data instance is an input + label pair\n","        train_inputs, train_labels = data\n","        print(train_inputs.shape)\n","        break\n","\n","        # Zero your gradients for every batch!\n","        optimizer.zero_grad()\n","\n","        # Make predictions for this batch\n","        train_outputs = model_E1(train_inputs)\n","\n","        # Computer Correctness\n","        train_predict = train_outputs.argmax(dim=1)\n","        train_correct += torch.eq(train_predict, train_labels).sum().item()\n","\n","        # Compute the loss and its gradients\n","        train_loss = criterion(train_outputs, train_labels)\n","        train_loss.backward()\n","\n","        # Adjust learning weights\n","        optimizer.step()\n","\n","        # Gather data and report\n","        running_loss += train_loss.item()\n","        total_loss += train_loss.item()\n","        if i % 1000 == 999:\n","            last_loss = running_loss / 1000 # loss per batch\n","            print(\"batch {} loss: {}\".format(i + 1, last_loss))\n","            print(\"total correct: {}, total accuracy: {}\".format(train_correct, train_correct/(batch_len * (i + 1))))\n","            tb_x = epoch_index * len(training_loader) + i + 1\n","            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n","            running_loss = 0.\n","\n","    return total_loss/(i + 1), train_correct/(batch_len * (i + 1))"],"metadata":{"id":"DTDeC_J9PWKX","executionInfo":{"status":"ok","timestamp":1697156198966,"user_tz":240,"elapsed":2,"user":{"displayName":"刘亦凡","userId":"09216365541576887674"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Initializing in a separate cell so we can easily add more epochs to the same run\n","timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","writer = SummaryWriter('runs/MNIST_trainer_{}'.format(timestamp))\n","epoch_number = 0\n","\n","EPOCHS = 5\n","\n","best_vloss = 1_000_000.\n","\n","train_losses = []\n","test_losses = []\n","train_accuracys = []\n","test_accuracys = []\n","\n","for epoch in range(EPOCHS):\n","    print('EPOCH {}:'.format(epoch_number + 1))\n","\n","    # Make sure gradient tracking is on, and do a pass over the data\n","    model_E1.train(True)\n","    avg_train_loss, train_accuracy = train_one_epoch(epoch_number, writer)\n","    train_losses.append(avg_train_loss)\n","    train_accuracys.append(train_accuracy)\n","\n","    # We don't need gradients on to do reporting\n","    model_E1.train(False)\n","\n","    test_total_loss = 0.0\n","    test_correct = 0\n","    for i, vdata in enumerate(test_loader):\n","        test_inputs, test_labels = vdata\n","        test_outputs = model_E1(test_inputs)\n","\n","        # test loss\n","        test_loss = criterion(test_outputs, test_labels)\n","        test_total_loss += test_loss.item()\n","\n","        # test correctness\n","        test_predict = test_outputs.argmax(dim=1)\n","        test_correct += torch.eq(test_predict, test_labels).sum().float().item()\n","\n","    avg_test_loss = test_total_loss/(i + 1)\n","    test_accuracy = test_correct/(batch_len * (i + 1))\n","    test_losses.append(avg_test_loss)\n","    test_accuracys.append(test_accuracy)\n","    print(\"Train Loss: {}, Test Loss: {}\".format(avg_train_loss, avg_test_loss))\n","    print(\"Train Accuracy: {}, Test Accuracy: {}\".format(train_accuracy, test_accuracy))\n","\n","    # Log the running loss averaged per batch\n","    # for both training and validation\n","    writer.add_scalars(\"Training vs. Validation Loss\", {\"Training\": avg_test_loss, \"Validation\": avg_test_loss}, epoch_number + 1)\n","    writer.flush()\n","\n","    # Track best performance, and save the model's state\n","    if avg_test_loss < best_vloss:\n","        best_vloss = avg_test_loss\n","        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n","        torch.save(model_E1.state_dict(), model_path)\n","\n","    epoch_number += 1\n","\n","writer.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MS_RiyxKQ7-n","outputId":"6ed95a21-8c54-4b03-a80a-bc3bfd5aa229"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EPOCH 1:\n","torch.Size([4, 1, 32, 32])\n","Train Loss: 0.0, Test Loss: 2.3025168307304384\n","Train Accuracy: 0.0, Test Accuracy: 0.1135\n","EPOCH 2:\n","torch.Size([4, 1, 32, 32])\n","Train Loss: 0.0, Test Loss: 2.3025168307304384\n","Train Accuracy: 0.0, Test Accuracy: 0.1135\n","EPOCH 3:\n","torch.Size([4, 1, 32, 32])\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10,5))\n","plt.title(\"Training Loss\")\n","plt.plot(range(1,EPOCHS+1), train_losses)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.show()"],"metadata":{"id":"Tt8W7C2zNf1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,5))\n","plt.title(\"Test Loss\")\n","plt.plot(range(1,EPOCHS+1), test_losses)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.show()"],"metadata":{"id":"0zd-Edty4WW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,5))\n","plt.title(\"Training Accuracy\")\n","plt.plot(range(1,EPOCHS+1), train_accuracys)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()"],"metadata":{"id":"4EqD1eau37T4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,5))\n","plt.title(\"Test Accuracy\")\n","plt.plot(range(1,EPOCHS+1), test_accuracys)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()"],"metadata":{"id":"hwoH1ScW4P8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Hflip_test_data = datasets.MNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=torchvision.transforms.Compose([torchvision.transforms.Resize(32),torchvision.transforms.RandomHorizontalFlip(p=1),torchvision.transforms.ToTensor()])\n",")\n","Vflip_test_data = datasets.MNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=torchvision.transforms.Compose([torchvision.transforms.Resize(32),torchvision.transforms.RandomVerticalFlip(p=1),torchvision.transforms.ToTensor()])\n",")\n","\n","Hflip_test_loader = torch.utils.data.DataLoader(Hflip_test_data, batch_size=batch_len, shuffle=False, num_workers=2)\n","Vflip_test_loader = torch.utils.data.DataLoader(Vflip_test_data, batch_size=batch_len, shuffle=False, num_workers=2)"],"metadata":{"id":"rDg4s_2KoJER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Hflip_test_correct = 0\n","Vflip_test_correct = 0\n","\n","# We don't need gradients on to do reporting\n","model_E1.train(False)\n","\n","for i, vdata in enumerate(Hflip_test_loader):\n","    Hflip_test_inputs, Hflip_test_labels = vdata\n","    Hflip_test_outputs = model_E1(Hflip_test_inputs)\n","\n","    # test correctness\n","    Hflip_test_predict = Hflip_test_outputs.argmax(dim=1)\n","    Hflip_test_correct += torch.eq(Hflip_test_predict, Hflip_test_labels).sum().float().item()\n","\n","Hflip_test_accuracy = Hflip_test_correct/(batch_len * (i + 1))\n","\n","for j, vdata in enumerate(Vflip_test_loader):\n","    Vflip_test_inputs, Vflip_test_labels = vdata\n","    Vflip_test_outputs = model_E1(Vflip_test_inputs)\n","\n","    # test correctness\n","    Vflip_test_predict = Vflip_test_outputs.argmax(dim=1)\n","    Vflip_test_correct += torch.eq(Vflip_test_predict, Vflip_test_labels).sum().float().item()\n","\n","Hflip_test_accuracy = Hflip_test_correct/(batch_len * (i + 1))\n","Vflip_test_accuracy = Vflip_test_correct/(batch_len * (j + 1))"],"metadata":{"id":"S-H-cZOQH5Pr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Test Accuracy For Random Horizontal Flip: {}\".format(Hflip_test_accuracy))\n","print(\"Test Accuracy For Random Vertical Flip: {}\".format(Vflip_test_accuracy))"],"metadata":{"id":"snqZaHNdIRbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flip_name = (\"Horizontal Flip\", \"Vertical Flip\")\n","flip_accuracy = (Hflip_test_accuracy, Vflip_test_accuracy)\n","plt.figure(figsize=(5,5))\n","plt.bar(flip_name, flip_accuracy, width = 0.4)\n","plt.xlabel(\"Flip Type\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()"],"metadata":{"id":"dcToWrL1ISQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Var001_test_data = datasets.MNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=torchvision.transforms.Compose([torchvision.transforms.Resize(32),torchvision.transforms.ToTensor(),torchvision.transforms.Lambda(lambda x : x + 0.1 * torch.randn_like(x))])\n",")\n","Var01_test_data = datasets.MNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=torchvision.transforms.Compose([torchvision.transforms.Resize(32),torchvision.transforms.ToTensor(),torchvision.transforms.Lambda(lambda x : x + torch.sqrt(0.1) * torch.randn_like(x))])\n",")\n","Var1_test_data = datasets.MNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=torchvision.transforms.Compose([torchvision.transforms.Resize(32),torchvision.transforms.ToTensor(),torchvision.transforms.Lambda(lambda x : x + torch.randn_like(x))])\n",")\n","\n","Var001_test_loader = torch.utils.data.DataLoader(Var001_test_data, batch_size=batch_len, shuffle=False, num_workers=2)\n","Var01_test_loader = torch.utils.data.DataLoader(Var01_test_data, batch_size=batch_len, shuffle=False, num_workers=2)\n","Var1_test_loader = torch.utils.data.DataLoader(Var1_test_data, batch_size=batch_len, shuffle=False, num_workers=2)"],"metadata":{"id":"CKK2kbfNUDmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Var001_test_correct = 0\n","Var01_test_correct = 0\n","Var1_test_correct = 0\n","\n","# We don't need gradients on to do reporting\n","model_E1.train(False)\n","\n","for i, vdata in enumerate(Var001_test_loader):\n","    Var001_test_inputs, Var001_test_labels = vdata\n","    Var001_test_outputs = model_E1(Var001_test_inputs)\n","\n","    # test correctness\n","    Var001_test_predict = Var001_test_outputs.argmax(dim=1)\n","    Var001_test_correct += torch.eq(Var001_test_predict, Var001_test_labels).sum().float().item()\n","\n","for k, vdata in enumerate(Var1_test_loader):\n","    Var1_test_inputs, Var1_test_labels = vdata\n","    Var1_test_outputs = model_E1(Var1_test_inputs)\n","\n","    # test correctness\n","    Var1_test_predict = Var1_test_outputs.argmax(dim=1)\n","    Var1_test_correct += torch.eq(Var1_test_predict, Var1_test_labels).sum().float().item()\n","\n","for j, vdata in enumerate(Var01_test_loader):\n","    Var01_test_inputs, Var01_test_labels = vdata\n","    Var01_test_outputs = model_E1(Var01_test_inputs)\n","\n","    # test correctness\n","    Var01_test_predict = Var01_test_outputs.argmax(dim=1)\n","    Var01_test_correct += torch.eq(Var01_test_predict, Var01_test_labels).sum().float().item()\n","\n","Var001_test_accuracy = Var001_test_correct/(batch_len * (i + 1))\n","Var01_test_accuracy = Var01_test_correct/(batch_len * (j + 1))\n","Var1_test_accuracy = Var1_test_correct/(batch_len * (k + 1))"],"metadata":{"id":"1HSQilubUFl2"},"execution_count":null,"outputs":[]}]}